

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>1.1. Framework &mdash; BRAILS 2.0.0 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/panels-bootstrap.5fd3999ee7762ccc51105388f4a9d115.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sphinxcontrib-images/LightBox2/lightbox2/dist/css/lightbox.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinxcontrib-images/LightBox2/lightbox2/dist/js/lightbox-plus-jquery.min.js"></script>
        <script src="../../_static/sphinxcontrib-images/LightBox2/lightbox2-customize/jquery-noconflict.js"></script>
        <script async="async" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="1.2. Modules" href="modulesTheory.html" />
    <link rel="prev" title="1. Theory and Implementation" href="theory.html" />
<script src="https://cdn.jsdelivr.net/npm/vega@5.12.1"></script>
<script src="https://cdn.jsdelivr.net/npm/vega-lite@4.13.1"></script>
<script src="https://cdn.jsdelivr.net/npm/vega-embed@6.8.0"></script>
<style media="screen">.vega-actions a {margin-right: 5px;}</style>
<link href="../../_static/css/bootstrap.css" rel="stylesheet">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #F2F2F2" >
          
<a href="https://simcenter.designsafe-ci.org/" style="margin-bottom: 0px;">
  <img src="../../_static/SimCenter_logo.png" class="logo" alt="Org-Logo" />
</a>
<hr style="margin: 0px;">

  <a href="../../index.html">


  <img src="../../_static/SimCenter_BRAILS_logo_solo.png" class="logo" alt="Logo"/>
</a>


  
  



<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>


        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">About</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../about/about.html">1. About</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about/acknowledgements.html">2. Acknowledgements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about/license.html">3. Copyright and License</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about/cite.html">4. How to Cite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about/glossary.html">5. Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about/abbreviations.html">6. Abbreviations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about/releasenotes.html">7. Release Notes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Manual</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user_manual/installation.html">1. Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_manual/userGuide.html">2. User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_manual/troubleshooting.html">3. Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_manual/examples.html">4. Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_manual/bugs.html">5. Bugs &amp; Feature Requests</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Technical Manual</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="theory.html">1. Theory and Implementation</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">1.1. Framework</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#extracting-building-information-from-images">Extracting building information from images</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-fusion">Data fusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-enhancement">Data enhancement</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="modulesTheory.html">1.2. Modules</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="vnv.html">2. Validation</a></li>
<li class="toctree-l1"><a class="reference internal" href="understand.html">3. Generalization</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">BRAILS</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="theory.html"><span class="section-number">1. </span>Theory and Implementation</a> &raquo;</li>
        
      <li><span class="section-number">1.1. </span>Framework</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/common/technical_manual/framework.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="framework">
<span id="lbl-framework"></span><h1><span class="section-number">1.1. </span>Framework<a class="headerlink" href="#framework" title="Permalink to this headline">¶</a></h1>
<p>This section presents a framework that works as an abstraction providing generic functionalities and can be selectively
changed by additional user-written codes with user-provided input data,
thus providing a region-specific building information harvesting tool.
The framework provides a standard way to create realistic building inventory databases and it is a universal, reusable environment that provides particular functionalities facilitating regional-scale building information modeling.</p>
<p>As shown in <a class="reference internal" href="#brailspipeline"><span class="std std-numref">Fig. 1.1.1</span></a>, the framework consists of two steps: data fusion and data enhancement.</p>
<div class="align-center figure" id="id70">
<span id="brailspipeline"></span><img alt="../../_images/pipeline.png" src="../../_images/pipeline.png" />
<p class="caption"><span class="caption-number">Fig. 1.1.1 </span><span class="caption-text">Framework</span><a class="headerlink" href="#id70" title="Permalink to this image">¶</a></p>
</div>
<p>Due to the complexity and the size of source data and the cost to collect it,
building information at the regional-scale is usually not able to be inferred
from a single resource but multiple resources, such as images, point clouds,
property tax records, crowd sourcing map, etc. And these data usually belong
to different owners and stored in different formats.
The framework combines these sources using data fusion (or data integration),
which is the process of integrating multiple building information data to produce
more consistent, accurate, and useful information than that provided by any
individual data source.
The expectation is that fused building information data is more informative
and synthetic than the original data.</p>
<p>The product of data fusion is an initial building inventory,
in which a significant amount of building information is missing because of data scarcity.
For example, crowd sourcing maps have issues with completeness, especially for rural regions.
They are more complete in densely urbanized areas and targeted areas of humanitarian
mapping intervention. Similarly, a significant amount of property tax assessment
records are missing from administrative databases. The incompleteness issue is
common for almost all data sources. In this framework, the missing values will
be filled by modellings performed based on the incomplete initial database.</p>
<div class="section" id="extracting-building-information-from-images">
<h2>Extracting building information from images<a class="headerlink" href="#extracting-building-information-from-images" title="Permalink to this headline">¶</a></h2>
<p>An initial building inventory, containing basic indexing information such as addresses or coordinates of individual buildings,
and other basic descriptions such as year built and structure type, has been created in the previous section.
Based on the indexing information, satellite or street view images of each building can be retrieved using Google Maps API.
Then, the deep learning technique ConvNet is utilized to extract building features
(that don’t exist in the initial database) from these images.
ConvNet is a class of deep neural networks inspired by biological processes in that the connectivity pattern
between neurons resembles the organization of the animal visual cortex <span id="id1">[<a class="reference internal" href="#id40" title="David H Hubel and Torsten N Wiesel. Receptive fields and functional architecture of monkey striate cortex. The Journal of physiology, 195(1):215–243, 1968.">HW68</a>]</span> and
is most commonly applied to analyzing images.
Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field.
The receptive fields of different neurons partially overlap such that they cover the entire visual field.</p>
<p>ConvNet is a supervised learning algorithm, which means the images need to be labeled for training.
Therefore the most important part is to build a labeled dataset.
OSM is a platform hosting real world infrastructure information labeled by human.
For a typical building, the information might be found in OSM includes: height, number of stories, structure type,
exterior material, footprint shape, usage, etc. These information are the valuable source for describing the built environment.
However, only a limit number of buildings are labeled in OSM.
In this study, the investigators propose to harvest these labels and associate them with images to build a database for deep learning.
The ConvNets trained on these database are used to predict building properties
when given any images containing unseen and unlabeled buildings.
This way, as long as the satellite/street view images of a city can be obtained,
a database of building information can be created. The pipeline to extract a specific building property
from images is listed as following:</p>
<ol class="arabic simple">
<li><p>Identify a visually comprehensible building property (e.g., exterior construction material) that is intended to be extracted.</p></li>
<li><p>Retrieve satellite/street view images of individual buildings from Google Map API</p></li>
<li><p>Label the retrieved images using tags (e.g., exterior construction material type) found in OSM</p></li>
<li><p>Train a ConvNet on the labeled images to classify between types</p></li>
<li><p>Apply the trained ConvNet to unlabeled satellite / street view images of buildings in the city of interest</p></li>
</ol>
<p>Repeat the above five steps for other building properties,
such as number of stories, structural type, etc., as long as they can be visually identified from images.
The ConvNet-identified building information is then merged into the initial database resulting in a more detailed inventory.</p>
<p>It should be noted that, due to reasons like heavy occlusions in images or bad viewpoints,
predictions from ConvNet are not always with acceptable confidence. To tackle this issue, in <a class="reference internal" href="#enhance"><span class="std std-ref">Data enhancement</span></a>,
a machine learning based approach will be employed to enhance the database.</p>
</div>
<div class="section" id="data-fusion">
<h2>Data fusion<a class="headerlink" href="#data-fusion" title="Permalink to this headline">¶</a></h2>
<p>In order to combine building information obtained form different sources,
a data fusion process is designed, as shown in <a class="reference internal" href="#brailsfusion"><span class="std std-numref">Fig. 1.1.2</span></a>.
There are two starting points of the data flow pipeline: one is the address list and the other one is the building footprints.</p>
<p>The address list is used as the index for querying the supporting data sources (e.g., OSM, tax records, other user provided data, etc.).
Once raw data is fetched from these sources, it will be filtered and cleaned to remove duplicated properties and then merged while missing values represented by place holders.</p>
<p>The building footprint is an important supporting data the framework relies on.
The method that is commonly used to get building footprints at a large scale is semantic segmentation on high-resolution
satellite maps <span id="id2">[<a class="reference internal" href="#id64" title="Benjamin Bischke, Patrick Helber, Joachim Folz, Damian Borth, and Andreas Dengel. Multi-task learning for segmentation of building footprints with deep neural networks. 2019 IEEE International Conference on Image Processing (ICIP), pages 1480–1484, 2019.">BHF+19</a>, <a class="reference internal" href="#id62" title="Weijia Li, Conghui He, Jiarui Fang, Juepeng Zheng, Haohuan Fu, and Le Yu. Semantic segmentation-based building footprint extraction using very high-resolution satellite images and multi-source gis data. Remote Sensing, 11(4):403, 2019.">LHF+19</a>, <a class="reference internal" href="#id63" title="Kang Zhao, Jungwon Kang, Jaewook Jung, and Gunho Sohn. Building extraction from satellite images using mask r-cnn with building boundary regularization. CVPR Workshops, pages 247–251, 2018.">ZKJS18</a>]</span>.
Since it is out of the focus of this study, instead of performing segmentation in the proposed framework on the fly,
the framework retrieves footprints from a dataset released by <span id="id3">[<a class="reference internal" href="#id55" title="Microsoft. US Building Footprints. URL: https://github.com/microsoft/USBuildingFootprints.">Mic</a>]</span>.
This dataset contains footprints for almost all buildings in the United States extracted from high-resolution satellite maps.
For regions outside of the United States, footprints can be inferred from satellite images using semantic segmentation methods in aforementioned  references.
Each geotagged address has a unique coordinate, therefore can be merged with corresponding building footprints.</p>
<p>Using address as the index, the aforementioned filtered and cleaned basic building information retrieved from
multiple data sources can now be merged into the main stream of the data flow pipeline. For buildings with missing information,
satellite and street view images of each building are then retrieved and fed into pretrained ConvNets,
and predictions on the building features such as number of stories, roof types, etc., can be obtained.
Merging the predicted values into the data stream results in the initial building database.</p>
<div class="align-center figure" id="id71">
<span id="brailsfusion"></span><img alt="../../_images/Fusion.png" src="../../_images/Fusion.png" />
<p class="caption"><span class="caption-number">Fig. 1.1.2 </span><span class="caption-text">Framework</span><a class="headerlink" href="#id71" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="data-enhancement">
<span id="enhance"></span><h2>Data enhancement<a class="headerlink" href="#data-enhancement" title="Permalink to this headline">¶</a></h2>
<p>Note that, after the fusion, the initial building database is still incomplete.
Some of the reasons are:
firstly no data source is perfect and there are usually a considerable amount of missing items in them;
secondly some missing values, for example the year of construction of an individual building,
are either visually incomprehensible to a ConvNet,
or for some visually comprehensible features, for example the number of stories,
if the building is occluded by other objects, usually a tree or a car, in the image,
which happens quite often, the feature can not be predicted accurately by ConvNets.
These reasons leave gaps in the initial building database.
This section describes a machine learning - based data enhancement method to
fill the gaps in the data.</p>
<p>Rather than merely a random assortment of objects in space,
landscapes, natural resources, the human built environment,
and other objects on Earth have orders, which can be described using a
spatial patterns - a perceptual structure, placement,
or arrangement of objects and the space in between those objects.
Patterns may be recognized because of their distance,
maybe in a line or by a clustering of points, and other arrangement types.</p>
<p>Such kind of spatial patterns, i.e., the arrangement of individual buildings
in space and the geographic relationships among them, exist in the distribution of buildings, too.
Buildings, when built, usually have a relationship between each other, i.e.,
one building is located at a specific location is usually because of another.
They can be clustered or dispersed based on their attributes,
such as building type, value, construction material, etc., which are usually the
manifestation of the demographic characteristics of neighborhoods,
such as household income or race.
For example, as the city shown in the map <a class="reference internal" href="#brailsmapsf"><span class="std std-numref">Fig. 1.1.3</span></a>,
there are areas denser with buildings than others and clusters of
certain types of building are easy to be found in certain regions.</p>
<div class="align-center figure" id="id72">
<span id="brailsmapsf"></span><img alt="../../_images/mapSF.png" src="../../_images/mapSF.png" />
<p class="caption"><span class="caption-number">Fig. 1.1.3 </span><span class="caption-text">Satellite view of buildings in San Francisco</span><a class="headerlink" href="#id72" title="Permalink to this image">¶</a></p>
</div>
<p>The capability of evaluating spatial patterns is a prerequisite to understanding the
complicated spatial processes underlying the distribution of a phenomenon.</p>
<p>In spatial statistics, the semivariogram is a function describing the degree of spatial
dependence of a spatial random field or stochastic process.
As such, statistics of spatial semivariogram provide a useful indicator of spatial patterns.
Semivariogram is essentially  a meassure of the degree of dissimilarity between
observations as a function of distance. It equals to half the variance of two
random variables separated by a vector distance <span class="math notranslate nohighlight">\(\boldsymbol{h}\)</span>
<span id="id4">[<a class="reference internal" href="#id65" title="Pierre Goovaerts. Geostatistics for natural resources evaluation. Oxford University Press on Demand, 1997.">Goo97</a>, <a class="reference internal" href="#id45" title="Erik Vanmarcke. Random fields: analysis and synthesis. World Scientific, 2010.">Van10</a>, <a class="reference internal" href="#id41" title="C Wang and Q Chen. A hybrid geotechnical and geological data-based framework for multiscale regional liquefaction hazard mapping. Géotechnique, 68(7):614–625, 2018.">WC18</a>, <a class="reference internal" href="#id42" title="Chaofeng Wang, Qiushi Chen, Mengfen Shen, and C Hsein Juang. On the spatial variability of cpt-based geotechnical parameters for regional liquefaction evaluation. Soil Dynamics and Earthquake Engineering, 95:153–166, 2017.">WCSJ17</a>]</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-gamma">
<span class="eqno">(1.1.1)<a class="headerlink" href="#equation-eq-gamma" title="Permalink to this equation">¶</a></span>\[\gamma (\boldsymbol{h})= \frac{1}{2}Var[Z(\boldsymbol{\mu}) - Z(\boldsymbol{\mu}+\boldsymbol{h})]\]</div>
<p>where <span class="math notranslate nohighlight">\(Z(\boldsymbol{\mu})\)</span> is the observation at a spatial location <span class="math notranslate nohighlight">\(\mu\)</span>;
<span class="math notranslate nohighlight">\(Z(\boldsymbol{\mu}+\boldsymbol{h})\)</span> is the observation at a spatial location <span class="math notranslate nohighlight">\(\boldsymbol{\mu}+\boldsymbol{h}\)</span>.</p>
<p>It is expected that buildings far away from each other will are more different
than buildings that are close to each other. Because based on the first rule of
geography that things close together are more similar than things far apart,
semivariogram is generally low when two locations are close to each other
(i.e. observations at each point are likely to be similar to each other.
Typically, semivariogram increases as the distance between the locations
grows until at some point the locations are considered independent of each other
and semi-variance no longer increases.
In the case of buildings,
semivariograms will give measures of how much two buildings will vary in attributes
(such as height, number of stories, etc.) regarding to the distance between those samples.</p>
<p>Semivariogram function is employed to explore the spatial patterns of
different building properties within a selected region.
The results show that buildings were indeed built following certain spatial patterns.
As a demonstration, the spatial semivariograms of two building properties,
number of stories and year of construction, are plotted in <a class="reference internal" href="#numofstories-semivariogram"><span class="std std-numref">Fig. 1.1.4</span></a>
and <a class="reference internal" href="#yearofbuilt-semivariogram"><span class="std std-numref">Fig. 1.1.5</span></a>.
The horizontal axis represents the distance between a pair of buildings, while the vertical axis represents the dis-similarity of these buildings.
The semivariogram figures show that with the increase of the distance between any two buildings, the dis-similarity between them,
regrading to number of stories and the year of construction for an example, increased and then fluctuated. Apparently the incremental
relationship between the distance and dis-similarity is neither linear nor following any obvious rule.
Another note that deserves to be taken here is the curves revealed in <a class="reference internal" href="#numofstories-semivariogram"><span class="std std-numref">Fig. 1.1.4</span></a> and <a class="reference internal" href="#yearofbuilt-semivariogram"><span class="std std-numref">Fig. 1.1.5</span></a>
are city- or region-specific, i.e., the semivariogram curve may reflect the truth of the region being investigated, and may not be exactly
correct or directly applicable for describing another region.
In other words, the spatial dependence of building features are regional-specific and the semivariogram curves vary regionally.</p>
<div class="align-center figure" id="id73">
<span id="numofstories-semivariogram"></span><img alt="../../_images/correlation_numofstories.png" src="../../_images/correlation_numofstories.png" />
<p class="caption"><span class="caption-number">Fig. 1.1.4 </span><span class="caption-text">Spatial patterns of building information expressed in semivariogram of the number of stories (The horizontal axis represents the distance between a pair of buildings, while the vertical axis represents the dis-similarity of these buildings.) These curves are calculated based on a building dataset covering four coastal cities in the Atlantic County, New Jersey</span><a class="headerlink" href="#id73" title="Permalink to this image">¶</a></p>
</div>
<div class="align-center figure" id="id74">
<span id="yearofbuilt-semivariogram"></span><img alt="../../_images/correlation_yearbuilt.png" src="../../_images/correlation_yearbuilt.png" />
<p class="caption"><span class="caption-number">Fig. 1.1.5 </span><span class="caption-text">Spatial patterns of building information expressed in semivariogram of the year of construction (The horizontal axis represents the distance between a pair of buildings, while the vertical axis represents the dis-similarity of these buildings.) These curves are calculated based on a building dataset covering four coastal cities in the Atlantic County, New Jersey</span><a class="headerlink" href="#id74" title="Permalink to this image">¶</a></p>
</div>
<p>Since the semivariograms (<a class="reference internal" href="#numofstories-semivariogram"><span class="std std-numref">Fig. 1.1.4</span></a> and <a class="reference internal" href="#yearofbuilt-semivariogram"><span class="std std-numref">Fig. 1.1.5</span></a> )
clearly show there is a spatial pattern of the distribution of a certain building property,
there must be a function for mapping neighbor information <span class="math notranslate nohighlight">\(\boldsymbol{Z}_{p}\)</span> into <span class="math notranslate nohighlight">\(Z_{n}\)</span>.
This function can be constructed implicitly using a neural network.</p>
<p>Imagine a neighborhood consisting of three buildings, <a class="reference internal" href="#neighborhood"><span class="std std-numref">Fig. 1.1.6</span></a>.
Pretrained ConvNets can easily extract attributes of building at two ends,
such as number of stories, occupancy, structure type, etc.
However, for the building in the middle, which is heavily occluded by a tree in this case,
no information can be extracted from the image with a satisfying confidence.
However, it is possible to predict the features of the building in the middle based on the information of its neighbors,
because <a class="reference internal" href="#numofstories-semivariogram"><span class="std std-numref">Fig. 1.1.4</span></a> and <a class="reference internal" href="#yearofbuilt-semivariogram"><span class="std std-numref">Fig. 1.1.5</span></a>  indicates that
the attributes of buildings within a community are correlated with each other.
The correlations can be learned by neural networks using <a class="reference external" href="https://github.com/NHERI-SimCenter/SURF">SURF</a>.</p>
<div class="align-center figure" id="id75">
<span id="neighborhood"></span><img alt="../../_images/neighbor.png" src="../../_images/neighbor.png" />
<p class="caption"><span class="caption-number">Fig. 1.1.6 </span><span class="caption-text">A neighborhood street view</span><a class="headerlink" href="#id75" title="Permalink to this image">¶</a></p>
</div>
<p>As mentioned in the previous sections,
there is a significant portion of building information still missing from the
initial database or can not be extracted from images.
Using <a class="reference external" href="https://github.com/NHERI-SimCenter/SURF">SURF</a>,
the missing values can be predicted based on known values of neighboring buildings,
hence the gaps in the initial database are filled and the regional building information database is enhanced.
Details about how to do this can be found in the documentation of <a class="reference external" href="https://github.com/NHERI-SimCenter/SURF">SURF</a>.</p>
<p id="id7"><dl class="citation">
<dt class="label" id="id64"><span class="brackets"><a class="fn-backref" href="#id2">BHF+19</a></span></dt>
<dd><p>Benjamin Bischke, Patrick Helber, Joachim Folz, Damian Borth, and Andreas Dengel. Multi-task learning for segmentation of building footprints with deep neural networks. <em>2019 IEEE International Conference on Image Processing (ICIP)</em>, pages 1480–1484, 2019.</p>
</dd>
<dt class="label" id="id65"><span class="brackets"><a class="fn-backref" href="#id4">Goo97</a></span></dt>
<dd><p>Pierre Goovaerts. <em>Geostatistics for natural resources evaluation</em>. Oxford University Press on Demand, 1997.</p>
</dd>
<dt class="label" id="id69"><span class="brackets">HZRS16</span></dt>
<dd><p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 770–778. 2016.</p>
</dd>
<dt class="label" id="id40"><span class="brackets"><a class="fn-backref" href="#id1">HW68</a></span></dt>
<dd><p>David H Hubel and Torsten N Wiesel. Receptive fields and functional architecture of monkey striate cortex. <em>The Journal of physiology</em>, 195(1):215–243, 1968.</p>
</dd>
<dt class="label" id="id62"><span class="brackets"><a class="fn-backref" href="#id2">LHF+19</a></span></dt>
<dd><p>Weijia Li, Conghui He, Jiarui Fang, Juepeng Zheng, Haohuan Fu, and Le Yu. Semantic segmentation-based building footprint extraction using very high-resolution satellite images and multi-source gis data. <em>Remote Sensing</em>, 11(4):403, 2019.</p>
</dd>
<dt class="label" id="id55"><span class="brackets"><a class="fn-backref" href="#id3">Mic</a></span></dt>
<dd><p>Microsoft. US Building Footprints. URL: <a class="reference external" href="https://github.com/microsoft/USBuildingFootprints">https://github.com/microsoft/USBuildingFootprints</a>.</p>
</dd>
<dt class="label" id="id66"><span class="brackets">SVI+16</span></dt>
<dd><p>Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 2818–2826, 2016.</p>
</dd>
<dt class="label" id="id45"><span class="brackets"><a class="fn-backref" href="#id4">Van10</a></span></dt>
<dd><p>Erik Vanmarcke. <em>Random fields: analysis and synthesis</em>. World Scientific, 2010.</p>
</dd>
<dt class="label" id="id41"><span class="brackets"><a class="fn-backref" href="#id4">WC18</a></span></dt>
<dd><p>C Wang and Q Chen. A hybrid geotechnical and geological data-based framework for multiscale regional liquefaction hazard mapping. <em>Géotechnique</em>, 68(7):614–625, 2018.</p>
</dd>
<dt class="label" id="id68"><span class="brackets">Wan20</span></dt>
<dd><p>Chaofeng Wang. Occupancy test. December 2020. URL: <a class="reference external" href="https://doi.org/10.5281/zenodo.4386991">https://doi.org/10.5281/zenodo.4386991</a>, <a class="reference external" href="https://doi.org/10.5281/zenodo.4386991">doi:10.5281/zenodo.4386991</a>.</p>
</dd>
<dt class="label" id="id42"><span class="brackets"><a class="fn-backref" href="#id4">WCSJ17</a></span></dt>
<dd><p>Chaofeng Wang, Qiushi Chen, Mengfen Shen, and C Hsein Juang. On the spatial variability of cpt-based geotechnical parameters for regional liquefaction evaluation. <em>Soil Dynamics and Earthquake Engineering</em>, 95:153–166, 2017.</p>
</dd>
<dt class="label" id="id67"><span class="brackets">Wan19</span></dt>
<dd><p>Charles Wang. Random satellite images of buildings. October 2019. URL: <a class="reference external" href="https://doi.org/10.5281/zenodo.3521067">https://doi.org/10.5281/zenodo.3521067</a>, <a class="reference external" href="https://doi.org/10.5281/zenodo.3521067">doi:10.5281/zenodo.3521067</a>.</p>
</dd>
<dt class="label" id="id44"><span class="brackets">WYM+19</span></dt>
<dd><p>Charles Wang, Qian Yu, Frank McKenna, Barbaros Cetiner, Stella X. Yu, Ertugrul Taciroglu, and Kincho H. Law. Nheri-simcenter/brails: v1.0.1. October 2019. URL: <a class="reference external" href="https://doi.org/10.5281/zenodo.3483208">https://doi.org/10.5281/zenodo.3483208</a>, <a class="reference external" href="https://doi.org/10.5281/zenodo.3483208">doi:10.5281/zenodo.3483208</a>.</p>
</dd>
<dt class="label" id="id63"><span class="brackets"><a class="fn-backref" href="#id2">ZKJS18</a></span></dt>
<dd><p>Kang Zhao, Jungwon Kang, Jaewook Jung, and Gunho Sohn. Building extraction from satellite images using mask r-cnn with building boundary regularization. <em>CVPR Workshops</em>, pages 247–251, 2018.</p>
</dd>
</dl>
</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="modulesTheory.html" class="btn btn-neutral float-right" title="1.2. Modules" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="theory.html" class="btn btn-neutral float-left" title="1. Theory and Implementation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, The Regents of the University of California.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
  
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>


</body>
</html>